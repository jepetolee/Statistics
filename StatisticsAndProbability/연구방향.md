# 연구방향

## 짧은 발성에서 SV

* 기존 해결 방향 중에 model fusion을 통한 SV 성능을 향상시키는 연구가 존재
* fusion 기법 문제점
  * fusion 기법은 여러가지 독립된 모델을 병렬적으로 사용하다보니 연산량 증가에 비해 성능 향상이 크지 않음. 즉, high cost하다.
  * 각 모델은 서로다른 test utt에 대해 서로 다른 예측 -> 그러나 test set은 label이 없기 때문에 어떤 모델을 쓸지 선택하는 것은 어려운 문제 (test set에서 모델별 가중치를 어떻게 설정할지 알 수 없음)
* 따라서 cost를 줄여 Short-duration(Sd) SV task에 적합한 fusion 기법을 제안하고자 함
  * *Q: 과연 이 기법이 Sd task에만 적절한 것일까? 본 연구는 low cost fusion 기법에 대한 연구에 더 가까움. 그렇다면 연구 분야를 SdSV에 한정하는게 아니고 SV 전체에 대해서 적용도 가능하지 않을까? -----> `speaker verification and knowledge distillation`*
  * *기존에 존재하는 fusion 기법에는 뭐가 있을까?*
    * [1,2] i-vector + DNN embedding (기존)
    * **[3] Classifier fusion method** (기존에 사용했던 emsemble 기법들에 대한 설명 포함)
* 기존 cost를 줄이는 기법
  * Model Compression: Weight Pruning ...
  * Quantization: Binarization, ...
  * **Knowledge Distillation**
    * **앙상블(Ensemble) 기법을 통해 학습된 다수의 큰 네트워크들로부터 작은 하나의 네트워크에 지식을 전달할 수 있는 방법론**
* Knowledge Distillation은 어떤 방식으로 사용되는지? [4]
  * ![image-20220401162059990](C:\Users\gustj\AppData\Roaming\Typora\typora-user-images\image-20220401162059990.png)
  * 발표 논문(CVPR)에서 사용한 방식은 (a) 방식을 채택함

## idea

![image-20220412112910862](C:\Users\gustj\AppData\Roaming\Typora\typora-user-images\image-20220412112910862.png)

* 발성 길이별  teacher & multi-teacher learning
* 자동차의 위치를 찾아내는 작업 == 화자 정보 위치를 찾아내는 작업
  * 긴 발성에 잘 동작하는 모델에 짧은 발성이 들어가면 성능 하락
  * 긴 발성 성능과 짧은 발성의 성능은 서로 연관성 있음 - 곱연산 불가능







1. Deeply Fused Speaker Embeddings for Text-Independent Speaker Verification (2018)
2. Generative X-Vectors for Text-Independent Speaker Verification (2018)
3. Sample-specific late classifier fusion for speaker verification (2017)
4. **Refine Myself by Teaching Myself : Feature Refinement via Self-Knowledge Distillation(2021)**